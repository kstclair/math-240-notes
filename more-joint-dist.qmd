# More with joint and conditional distributions  {#sec-more-joint-dist}

We will explore strategies for deriving the distribution of a function of a two (or more) random variables. Then we return to thinking conditionally (first introduced in @sec-think-cond) to see how to define a conditional pdf/pmf probability model. 

Make sure to review our [week 8 schedule](https://docs.google.com/spreadsheets/d/1RrtZ7JiEBz_r4XMC7Eib2ObRZpGSXkg2fVV1h8YrnUw/edit?usp=sharing).



## Day 21 {.unnumbered}

- What to read: Read sections 8.2, 8.3, 8.4

- Learning objectives: These sections will help you
  - derive the pdf of a minimum or maximum 
  - derive the pmf/pdf of a sum of two independent RV using a convolution of functions
  
- After reading, take reading [quiz 17](https://forms.gle/KsqVyoKtsQLFrZcs5)

  
### Sections 8.2 {.unnumbered}

Key terminology to know:

  - [ ] order statistics
  - [ ] inequalities for mins and maxs

Comments: 

- Focus on material involving the min $X_{(1)}$ and max $X_{(n)}$ order statistics. 
- You can skim the proof of the beta distribution for general $X_{(k)}$ for $1 < k < n$ when your sample is iid uniform. (pages 333-334)


### Sections 8.3 {.unnumbered}

Key terminology to know:

  - [ ] convolution of two functions

Comments: 

- The convolution method of finding the pdf of $X+Y$ is an example of a CDF transformation method (i.e. find the CDF then differentiate to find PDF)
- Review section 4.4.1 equation 4.7 for review of the analogous discrete version of 8.3. 


### Sections 8.4 {.unnumbered}

Comments: 

- This section reviews both how to use geometry to compute probabilities from uniform pdfs and use of the CDF method to find PDF for a function of two RV.
- You can skim 8.23 (Buffon's needle problem is fun but you won't necessarily see trig calculations in hw/exam problems!)


## Day 22 {.unnumbered}

- What to read: Read sections 4.8, 9.1, 9.2

- Learning objectives: These sections will help you
  - derive conditional pmf/pdf from joint and marginal models
  - apply Bayes formula to conditional pmf/pdf
  
- After reading, take reading [quiz 18](https://forms.gle/aoCcCr7x9s8o6Uhk8)

  
### Sections 4.3, 9.1, and 9.2  {.unnumbered}

Key terminology to know:

  - [ ] conditional pmf
  - [ ] conditional pdf
  - [ ] Bayes formula for pdf

Comments: 

- We will cover expected value in 4.8.1 on Day 23. 
- Take care when deriving conditional pmf/pdf. E.g the pdf $f(y \mid x)$ of $Y \mid X=x$ is a function of $y$ and $x$ is a constant. 




## Day 23 {.unnumbered}

- What to read: Read sections 9.3, 9.5

- Learning objectives: These sections will help you
  - Compute a conditional pmf/pdf's expected value and variance given a fixed value of the conditioning variable
  - Compute a marginal expected value or variance from a RV's conditional distribution
  
- After reading, take reading [quiz 19](https://forms.gle/AZmecHbJtKckUvkL6)

  
### Sections  9.3 and 9.5 {.unnumbered}

Key terminology to know:

  - [ ] Conditional expected value of $Y$ given $X=x$: $E(Y \mid X=x)$  (similar for flipping the roles of $X$ and $Y$)
  - [ ]  Conditional variance of $Y$ given $X=x$: $V(Y \mid X=x)$ (similar for flipping the roles of $X$ and $Y$)
  - [ ] LOTUS for conditional distributions
  - [ ]  Conditional expected value of $Y$ given $X$: $E(Y \mid X)$  (similar for flipping the roles of $X$ and $Y$)
  - [ ]  Conditional variance of $Y$ given $X$: $V(Y \mid X)$ (similar for flipping the roles of $X$ and $Y$)
  - [ ] Law of total expectation
  - [ ] Law of total variance
  

Comments: 

- For these sections, we are framing the conditional relation as "$Y$ given $X$". The same rules/def/ideas apply if we flip the roles of $Y$ and $X$, so don't get stuck on a particular formula - try to see how things would generalize to any RV. E.g. examples 9.13 and 9.14 switch up notation. 

- For these sections, we often frame the known information/models as being given the distribution (marginally) of $X$ and the conditional distribution of $Y$ given $X=x$ (or $X$). As already mentioned, we could flip the roles of the RVs and have the same general rules apply.

- I know these sections can get confusing since we are talking about very similar things with similar notation. One important idea, though, is whether a value (exp/var) is *fixed* (but maybe unknown, in formula form) or *random.* 
  - $E(Y \mid X=x)$ assumes that we know a value of $X=x$. The final answer may be a function of $x$, but $x$ is a fixed (not random) value. Like in Ex 9.12, $E(Y \mid X=0.6) = (0.6)/2=0.3$ is the expected value of Miguel's number if Riley picks $X=0.6$.
   - $E(Y \mid X) = g(X)$ keeps $X$ *random*. Why do this? We might want to know how $E(Y \mid X) = g(X)$ behaves over all possible, random, values of $X$.  Like in Ex 9.12, $E(Y \mid X) = X/2$ tells us that the random variable $X/2$ models the  expected value of Miguel's number as a function of the random value $X$ of Riley's pick.
   
- The law of total expectation tells us the "average" value of $E(Y \mid X)$ over all values of $X$ ("averaged" over the pdf/pmf of $X$). It is a way to get the unconditional expectation of $Y$ without having to compute the marginal distribution of $Y$.
  - e.g. in Ex 9.12, $E(Y \mid X) = X/2$ tells us the  expected value of Miguel's number as a function of the random value $X$. If $X \sim Unif(0,1)$, then $E(X) = 0.5$ and the unconditional expectation for Miguel's number is $E(Y) = E_x(E(Y \mid X)) = E_x(X/2) = (0.5)/2 = 0.25$. As a gut check, we should expect the expectation for Miguel to be lower than Riley because Riley's number always dictates the upper bound on Miguel's number.
  
- The law of total variance (LOTV) is similar in spirit to the law of total expectation, that we want to understand the variance of $Y$ unconditionally when what we are given is the conditional variance. But, the equation for the LOTV is more complex because the variance term is more complex: it involves taking an expectation of squared deviation of the RV from its expectation.  The LOTV is computed with two parts:
  - $E_x(V(Y \mid X))$: "average" the conditional variance (RV) $V(Y\mid X)$ over all values of $X$ ("averaged" over the pdf/pmf of $X$).
  - $V_x(E(Y \mid X))$: measure the variation of the conditional expected value (RV) $E(Y\mid X)$ over all values of $X$ (variation with respect to the pdf/pmf of $X$).
  


  