% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  letterpaper,
]{scrbook}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother

\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={math-240-notes},
  pdfauthor={Katie St.~Clair},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}


\title{math-240-notes}
\author{Katie St.~Clair}
\date{2024-11-13}

\begin{document}
\frontmatter
\maketitle

\renewcommand*\contentsname{Table of contents}
{
\setcounter{tocdepth}{2}
\tableofcontents
}

\mainmatter
\bookmarksetup{startatroot}

\chapter*{Math 240 Probability}\label{math-240-probability}
\addcontentsline{toc}{chapter}{Math 240 Probability}

\markboth{Math 240 Probability}{Math 240 Probability}

This is a reading guide resource for students in my Math 240 Probability
class taught at Carleton College (Northfield, MN). Our textbook is
Wagaman (2021). You can find this
\href{https://bridge.primo.exlibrisgroup.com/permalink/01BRC_INST/11mmqid/cdi_askewsholts_vlebooks_9781119692416}{book
online in our library}.

\bookmarksetup{startatroot}

\chapter{First principles of probability}\label{sec-first-p}

First principles cover sections in chapter 1 of Wagaman (2021). Make
sure to review our
\href{https://docs.google.com/spreadsheets/d/1QKQFfuuDzFUpOdjMvmHWZpR6zUzoxd9UjE83vA0H1Fs/edit?usp=sharing}{week
1 schedule}.

\section*{Day 1}\label{day-1}
\addcontentsline{toc}{section}{Day 1}

\markright{Day 1}

\begin{itemize}
\item
  What to read: Read sections 1.1, 1.2, 1.3, 1.4, and 1.8 to see the
  generalization of property \#3.
\item
  Learning objectives: These sections will help you

  \begin{itemize}
  \tightlist
  \item
    define basic probability and set theory terminology
  \item
    define fundamental properties of probability
  \end{itemize}
\end{itemize}

\subsection*{Section 1.1}\label{section-1.1}
\addcontentsline{toc}{subsection}{Section 1.1}

Key terminology to know:

\begin{itemize}
\tightlist
\item[$\square$]
  Sample space \(\Omega\)
\item[$\square$]
  Outcome \(\omega\): this can also be called an element
\item[$\square$]
  Event \(A\), \(B\), \ldots{}
\end{itemize}

Comments:

The sample space for a random experiment is the list of all possible
outcomes. To define a sample space, start by thinking about how to
define a couple individual outcomes and then generalize this to all
outcomes. If there are a large or infinite number of outcomes, then you
can use a ``\ldots{}'' to show that a pattern will continue (like
Examples 1.3 and 1.4).

There may not be a unique way to define a sample space and outcomes for
a random experiment. For example, one may chose to define the sample
space for Example 1.3 as just the number of votes for Yolanda
\[\Omega = \{ 0, 1, 2, \dotsc, 999, 1000\}\]. But all the outcomes in a
sample space must define unique possibilities for a random experiment
(i.e.~they are mutually exclusive, section 1.4).

``Complex'' random experiments are often composed of simpler
experiments. Example 1.2 is an example of this as we define one outcome
for two dice rolls as the joint outcome of two individual die rolls.
Similar for the sample space for three coin flips. In scenarios where
the individual simpler experiments have equally likely outcomes, the
outcomes in the more complex sample space (e.g.~two dice rolls, three
coin flips) are also equally likely outcomes.

\subsection*{Section 1.2-1.3}\label{section-1.2-1.3}
\addcontentsline{toc}{subsection}{Section 1.2-1.3}

Key ideas to know:

\begin{itemize}
\tightlist
\item[$\square$]
  relative frequency interpretation of probability
\item[$\square$]
  probability function and its essential properties
\end{itemize}

Comments:

Make sure you can put the probability function properties into words:
(1) means probabilities can't be negative, (2) means something in the
sample space has to happen with probability 1 and (3) means the
probability of an event is just the sum of the probabilities of outcomes
that make up that event.

\subsection*{Section 1.4 + 1.8}\label{section-1.4-1.8}
\addcontentsline{toc}{subsection}{Section 1.4 + 1.8}

Key set theory ideas to know

\begin{itemize}
\tightlist
\item[$\square$]
  complement \(A^c\) (not \(A\))
\item[$\square$]
  union \(A \cup B\) (at least one \(A\) or \(B\) or both)
\item[$\square$]
  intersection \(A \cap B = AB\) (both \(A\) and \(B\))
\item[$\square$]
  \((AB)^c = A^c \cup B^c\) (at most one)
\item[$\square$]
  \((A \cup B)^c = A^cB^c\) (neither)
\item[$\square$]
  \(AB^c\) (\(A\) but not \(B\))
\item[$\square$]
  subset \(\subseteq\)
\item[$\square$]
  mutually exclusive/disjoint events
\item[$\square$]
  Venn diagram
\item[$\square$]
  empty set \(\emptyset\)
\end{itemize}

Comments:

The first six ideas create a new event out of one or more events (think
addition, subtraction, etc). Subset, mutually exclusive and Venn
diagrams tells us relational information about events (think less than,
etc). The empty set is a set that has no outcomes (think zero).

Key probability properties to know

\begin{itemize}
\tightlist
\item[$\square$]
  Addition rule for mutually exclusive events (only add probability of
  events when they are mutually exclusive)
\item[$\square$]
  General addition rule (events do not need to be mutually exclusive)
\item[$\square$]
  Complement rule
\end{itemize}

Comments:

The addition rule properties are used to find the probability of a
\emph{union} of two or more events. When events are mutually exclusive,
we just add the individual event probabilities. Make sure that you
assess whether events are mutually exclusive before simply adding their
probabilities.

When events are not mutually exclusive, you start by adding event
probabilities but then you need to subtract out the probability of the
overlap (intersection) between events. The inclusion/exclusion rule is
an extension of property (3) (eq. 1.3).

The complement rule along with the addition rule is useful for computing
the probability of neither \(A\) nor \(B\): \[
P(A^c B^c) = 1 - P(A \cap B)
\]

\section*{Day 2}\label{day-2}
\addcontentsline{toc}{section}{Day 2}

\markright{Day 2}

\begin{itemize}
\item
  What to read: Read sections 1.5 and 1.6
\item
  Learning objectives: These sections will help you

  \begin{itemize}
  \tightlist
  \item
    use the multiplication principle to count the number of outcomes in
    a sample space or event
  \item
    compute event probabilities using counting when outcomes are equally
    likely
  \item
    define and count permutations
  \end{itemize}
\item
  After reading, take reading
  \href{https://forms.gle/JeRauvsZLp4FqnUK6}{quiz 1}
\end{itemize}

\subsection*{Section 1.5}\label{section-1.5}
\addcontentsline{toc}{subsection}{Section 1.5}

Key idea to know:

\begin{itemize}
\tightlist
\item[$\square$]
  equally like outcomes
\end{itemize}

Comments:

Our definition of the probability function Section 1.3 means that the
probability of any event \(A\) is equal to the number of outcomes in
\(A\) divided by the total number of outcomes when all outcomes are
equally likely.

\subsection*{Section 1.6}\label{section-1.6}
\addcontentsline{toc}{subsection}{Section 1.6}

Key ideas to know:

\begin{itemize}
\tightlist
\item[$\square$]
  multiplication principle
\item[$\square$]
  permutation
\item[$\square$]
  counting the number of permutations of \(n\) unique/distinct objects
\item[$\square$]
  counting the number of permutations of size \(k\) from \(n\)
  unique/distinct objects
\item[$\square$]
  sampling with vs without replacement
\end{itemize}

Comments:

The outcomes counted by the multiplication principle describe a specific
ordering, or arrangement, of a random experiment. For example 1.13, the
outcomes in the sample space are found by fixing the exam (eg exams 1-4)
and randomly assigning one of five grades to each exam. There are
\(5\times5\times5\times5 = 5^4\) such outcomes in the sample space. The
complement of the event of interest is getting no A's and we must
describe outcomes in this set the same way that we described them in the
sample space (eg assigning one of four non-A grades to each exam). There
are \(4^4\) such ways to assign a non-A grade to each exam.

Permutations are always ordered arrangements of unique/distinct objects
or individual outcomes.

Example 1.14 shows a common technique in counting problems. One outcome
describes a specific (unique) arrangement of 15 books (eg positions 1-5
are top shelf, 6-10 middle, and 11-15 bottom). There are 15! such
arrangements in the sample space. The event of interest, all math books
on the bottom, uses both permutation counting and the multiplication
principle. Permutations counts the number of ways to arrange math books
(5!) and novels (10!). Each unique arrangement of math books can be
combined with each unique arrangement of novels. Hence the
multiplication principle is used to count the number of ways to get 10
novels on the top and middle shelves and 5 math books on the bottom:
\(10! \times 5!\).

One twist to problem 1.14: suppose the event of interest was simply that
the 5 math books were on the same shelf (ie they could be on the top,
middle or bottom shelves). Each of these three options,
\(A_{top}, A_{middle}, A_{bottom}\) for math book placement has
\(10! \times 5!\) ways to arrange the books and each of these shelf
positions \(A_{i}\) is mutually exclusive. Hence the probability that
the 5 math books are on the same shelf is found using the addition rule
for mutually exclusive events: \[
P(A_{top} \cup A_{middle} \cup A_{bottom}) = P(A_{top}) + P(A_{middle}) +  P(A_{bottom}) = 3 \times \dfrac{10! \times 5!}{15!}
\]

\section*{Day 3}\label{day-3}
\addcontentsline{toc}{section}{Day 3}

\markright{Day 3}

\begin{itemize}
\item
  What to read: Read sections 1.7
\item
  Learning objectives: These sections will help you

  \begin{itemize}
  \tightlist
  \item
    define and count combinations using the binomial coefficient
  \item
    use the binomial coefficient to count permutations of two types of
    objects (a binary sequence)
  \item
    distinguish between a permutation and a combination
  \end{itemize}
\item
  After reading, take reading
  \href{https://forms.gle/D5hXDdpzE5mcBLkaA}{quiz 2}
\end{itemize}

\subsection*{Section 1.7}\label{section-1.7}
\addcontentsline{toc}{subsection}{Section 1.7}

Key ideas to know:

\begin{itemize}
\tightlist
\item[$\square$]
  combination
\item[$\square$]
  correspondence between a binary sequence/list and an unordered
  subset/sample of size \(k\) from \(n\) unique objects
\item[$\square$]
  binomial coefficient
\end{itemize}

Comments:

We will be considering counting problems where outcomes are either
\emph{ordered} and \emph{unordered}. Make sure that your strategy for
solving a problem is consistent when counting sample space and event
outcomes (eg don't use an ordered strategy for one and an unordered
strategy for the other).

While you need to describe outcomes in an event and sample space of
interest in the same manner, you don't need to use the same counting
method to count outcomes in each. e.g.~Example 1.26 uses the
multiplication principle to count all possible ways to flip a coin 20
times while the binomial coefficient is used to count how many of these
outcomes contain example 10 H and 10 T.

The binomial coefficient, which counts both the number of unordered
subsets of unique objects and the number of binary sequences, can be
generlized to a multinomial coefficient. Example 1.15(ii) involves
picking 4 subsets of 13 cards from a deck of 52:
\(\frac{52!}{13!13!13!13!}\). Example 1.27(ii) also uses a multinomial
coefficient in the numerator: \(\frac{20!}{4!5!3!8!}\) counts the number
of ways to arrange 4 As, 5 Gs, 3 Ts, and 8 Cs.

Don't worry about the binomial theorem (and its proof) for now.

\bookmarksetup{startatroot}

\chapter{Thinking conditionally}\label{sec-think-cond}

Chapter 2 of Wagaman (2021) covers conditional probabilities and the
concept of independence.

Make sure to review Chapter~\ref{sec-first-p} before this chapter and
review our
\href{https://docs.google.com/spreadsheets/d/1e_yVbPgnfymWyik8cLP_5qgLQfekGSh_jvFxE7u53P0/edit?usp=sharing}{week
2 schedule}.

\section*{Day 4}\label{day-4}
\addcontentsline{toc}{section}{Day 4}

\markright{Day 4}

\begin{itemize}
\item
  What to read: Read sections 2.1, 2.2, 2.3
\item
  Learning objectives: These sections will help you

  \begin{itemize}
  \tightlist
  \item
    conceptualize and compute a conditional probability
  \item
    use the general multiplication rule to compute the probabilities of
    intersections
  \item
    use tree diagrams to organize conditional information and compute
    probabilities
  \end{itemize}
\item
  After reading, take reading
  \href{https://forms.gle/Gg8ZU3keRsw62JC38}{quiz 3}
\end{itemize}

\subsection*{Section 2.1 and 2.2}\label{section-2.1-and-2.2}
\addcontentsline{toc}{subsection}{Section 2.1 and 2.2}

Key terminology to know:

\begin{itemize}
\tightlist
\item[$\square$]
  conditional probability function and its essential properties
\end{itemize}

Comments:

Don't get bogged down in the notation choice used to define a
conditional probability, \(P(A \mid B)\). The big idea is that to
compute a conditional probability we compute the ratio of the
intersection probability (chance that both events occur) relative to the
probability of the event we are conditioning on (ie the event that has
occurred).

Don't worry about the simulation code in example 2.3.

The conceptual idea in 2.2 is most important, that a conditional
probability function \(P( \cdot \mid B)\) should have the same
properties as an unconditional probability \(P(\cdot)\). The only
difference is that the sample space for the conditional probability is
restricted to only outcomes that agree with the conditional information.

\subsection*{Section 2.3}\label{section-2.3}
\addcontentsline{toc}{subsection}{Section 2.3}

Key terminology to know:

\begin{itemize}
\tightlist
\item[$\square$]
  general multiplication rule for two or more events
\item[$\square$]
  tree diagram
\end{itemize}

Comments:

Tree diagrams are a useful visual tool for organizing information that
is conditional or sequential in nature. Venn diagrams are not useful for
organizing such information.

\section*{Day 5}\label{day-5}
\addcontentsline{toc}{section}{Day 5}

\markright{Day 5}

\begin{itemize}
\item
  What to read: Read sections 2.4 and 2.5
\item
  Learning objectives: These sections will help you

  \begin{itemize}
  \tightlist
  \item
    use the law of total probability to compute an unconditional
    probability from conditional information
  \item
    use Bayes rule to compute a ``flipped/inverted'' conditional
    probability
  \end{itemize}
\item
  After reading, take reading
  \href{https://forms.gle/qqLNKnNCf21jkaL96}{quiz 4}
\end{itemize}

\subsection*{Section 2.4}\label{section-2.4}
\addcontentsline{toc}{subsection}{Section 2.4}

Key terminology to know:

\begin{itemize}
\tightlist
\item[$\square$]
  sample space partition
\item[$\square$]
  law of total probability (LoTP)
\end{itemize}

Comments:

Recognize that the LoTP uses both the multiplication rule (to compute
intersections) and the addition rule for mutually exclusive events.

\subsection*{Section 2.5}\label{section-2.5}
\addcontentsline{toc}{subsection}{Section 2.5}

Key terminology to know:

\begin{itemize}
\tightlist
\item[$\square$]
  Bayes formula/rule
\item[$\square$]
  tree diagram
\end{itemize}

Comments:

Bayes formula is useful when we want to flip the direction of a
conditional probability, eg. we want \(P(B \mid A)\) but we are given
\(P(A \mid B)\) along with unconditional info about \(B\). But the
formula didn't appear from nothing, it is based on the original
definition of a conditional probability from 2.1: the numerator is the
multiplication rule and the denominator is the LoTP.

\section*{Day 6}\label{day-6}
\addcontentsline{toc}{section}{Day 6}

\markright{Day 6}

\begin{itemize}
\item
  What to read: Read sections 2.6
\item
  Learning objectives: These sections will help you

  \begin{itemize}
  \tightlist
  \item
    determine whether events are independent or dependent
  \item
    compute intersection probabilities \emph{if} events are independent
  \end{itemize}
\item
  After reading, take reading
  \href{https://forms.gle/oXD68yZtkMh7Lxv3A}{quiz 5}
\end{itemize}

\subsection*{Section 2.6}\label{section-2.6}
\addcontentsline{toc}{subsection}{Section 2.6}

Key terminology to know:

\begin{itemize}
\tightlist
\item[$\square$]
  independent and dependent events
\item[$\square$]
  mutual independence
\item[$\square$]
  multiplication rule for independent events
\end{itemize}

Comments:

Independence is proved (or disproved) by thinking conditionally: if the
occurrence of \(B\) doesn't affect the probability of \(A\), then these
events are independent.

If we know events are independent, then we can multiply unconditional
probabilities to compute intersections. A common error is that I see is
when the multiplication rule for independent events is used without
first checking the essential assumption of independence.

Don't worry about \(A\) before \(B\) results (ie they are interesting
but not a general rule you need to know).

\bookmarksetup{startatroot}

\chapter{Introduction to discrete random
variables}\label{sec-intro-disc-rv}

Chapter 3 of Wagaman (2021) introduces us to discrete random variables
and some common probability models used to describe these random
variables.

Make sure to review our
\href{https://docs.google.com/spreadsheets/d/1u4l1ho1V35AJJSlt2-UaOW_C8xn46YK5UqrQjajpDH4/edit?usp=sharing}{week
3 schedule}.

\section*{Day 7}\label{day-7}
\addcontentsline{toc}{section}{Day 7}

\markright{Day 7}

\begin{itemize}
\item
  What to read: Read sections 3.1, 3.2, 3.3, 3.4, plus the pmf
  definition on page 126
\item
  Learning objectives: These sections will help you

  \begin{itemize}
  \tightlist
  \item
    understand what a ``random variable'' (RV) is
  \item
    define a probability mass function and support set for a discrete RV
  \item
    understand whether discrete RV are independent
  \item
    get familiar with some common discrete RV types (uniform, Bernoulli,
    binomial, Poisson)
  \end{itemize}
\item
  After reading, take reading
  \href{https://forms.gle/gMDmvhPHPysXiPkK8}{quiz 6}
\end{itemize}

\subsection*{Section 3.1 and page 126}\label{section-3.1-and-page-126}
\addcontentsline{toc}{subsection}{Section 3.1 and page 126}

Key terminology to know:

\begin{itemize}
\tightlist
\item[$\square$]
  random variable
\item[$\square$]
  discrete random variable
\item[$\square$]
  probability mass function (pmf) of a RV
\item[$\square$]
  the (support) set \(S\) of a RV
\item[$\square$]
  uniform RV (know shorthand notation, pmf and support set)
\end{itemize}

Comments:

Pay special attention to the notation used for random variables (RV).
Uppercase letters (typically at the end of the English alphabet) are
used to denote the RV (which is random and doesn't have a fixed value)
while lower case are used to denote a fixed numeric value (which is
fixed even though there may not be a value specified).

The ``distribution'' of a random variable describes the probability
structure of the RV, so think pmf if you are asked to describe a
distribution. (ie what values of the RV are most likely, which ones are
less likely, etc) You can also describe a distribution of a RV by name
if it has a common pmf (eg ``The distribution of \(X\) is discrete
uniform.'')

The subsection ``Random variables as function'' explains how a RV
\(X(\cdot)\) takes in one or more outcomes \(\omega\) from the sample
space of an experiment and outputs a numeric value. This is our formal
definition of a RV but we typically won't be using the \(X(\omega)\)
notation for a RV. Instead we will just refer to RV as \(X\), \(Y\),
etc. But keep this underlying connection with the sample space in mind
as the term progresses (especially for discrete RV).

The uniform random variable box on page 96 is an example of a
probability mass function (pmf) and support set \(S\). Even though pmf
aren't formally defined until ch.~4, I find it useful to use that
language at the start of discrete RV discussions.

\subsection*{Section 3.2}\label{section-3.2}
\addcontentsline{toc}{subsection}{Section 3.2}

Key terminology to know:

\begin{itemize}
\tightlist
\item[$\square$]
  independent discrete random variables
\end{itemize}

Comments:

We can use either Equations 3.1 or 3.2 to define two independent
discrete RV. The general definition of independent random variables on
page 98 extends beyond discrete RV to continuous RV (which we will cover
soon). Skim this idea but our main focus right now is discrete RV.

\subsection*{Section 3.3 and 3.4}\label{section-3.3-and-3.4}
\addcontentsline{toc}{subsection}{Section 3.3 and 3.4}

Key terminology to know:

\begin{itemize}
\tightlist
\item[$\square$]
  Bernoulli RV (know shorthand notation, what it counts, pmf and support
  set)
\item[$\square$]
  Independent and identically distributed (i.i.d.)
\item[$\square$]
  Binomial RV (know shorthand notation, what it counts, pmf and support
  set)
\end{itemize}

Comments:

Bernoulli and binomial RV are extremely common types of RV so make sure
to carefully review these sections.

Your book describes a binomial RV as the sum of an {[}i.i.d.{]}
Bernoulli ``sequence'' of length \(n\). Another common way to describe
this is the use the phrasing ``trials'' instead of ``sequence'': a
binomial RV is the sum of \(n\) i.i.d. Bernoulli trials.

The R code on pages 102 and 104 will be talked about on day 8.

Skim Example 3.14 but we will focus on solving problems involving two RV
later on this term.

\section*{Day 8}\label{day-8}
\addcontentsline{toc}{section}{Day 8}

\markright{Day 8}

\begin{itemize}
\item
  What to read: Read sections 3.5
\item
  Learning objectives: These sections will help you

  \begin{itemize}
  \tightlist
  \item
    get familiar with the Poisson distribution
  \item
    review or get introduced to infinite series
  \end{itemize}
\item
  After reading, take reading
  \href{https://forms.gle/GkduDGR42NEbizfE6}{quiz 7}
\end{itemize}

\subsection*{Section 3.5}\label{section-3.5}
\addcontentsline{toc}{subsection}{Section 3.5}

Key terminology to know:

\begin{itemize}
\tightlist
\item[$\square$]
  Poisson RV (know shorthand notation, what it counts, pmf and support
  set)
\end{itemize}

Comments:

A Poisson RV is another RV that counts ``things'', make sure you can
distinguish settings where we should be modeling a count RV as a Poisson
RV vs.~a binomial RV (vs.~something else).

Series are a Calc BC or Math 210 topic that not all of you have seen. If
you haven't had prior exposure to these ideas please stop by drop-in
hours (or make an appointment) if you have questions! Appendix C at the
end of your book reviews some useful math/calc facts, including some
common series. You can use these facts without deriving them from
scratch.

Skim 3.5.1 and 3.5.2. Time permitting, I'll cover the proof connecting
the binomial and Poisson distributions from 3.5.2 class (it's fun!) but
it isn't essential course material.

\bookmarksetup{startatroot}

\chapter{More with discrete random variables}\label{sec-more-disc-rv}

Chapters 4 and 5 of Wagaman (2021) cover more properties and types of
discrete random variables.

Make sure to review Chapter~\ref{sec-intro-disc-rv} before these
chapters and review our
\href{https://docs.google.com/spreadsheets/d/1DcSJuVX8xt5VTa5RV1PMZRaRvEhlWqvQaN7IacuW-ww/edit?usp=sharing}{week
4 schedule}.

\section*{Day 10}\label{day-10}
\addcontentsline{toc}{section}{Day 10}

\markright{Day 10}

\begin{itemize}
\item
  What to read: Read sections 4.1, 4.2, 4.4, 4.5 but review the comments
  below on what to skim/skip.
\item
  Learning objectives: These sections will help you

  \begin{itemize}
  \tightlist
  \item
    conceptualize and compute the expected value of a discrete random
    variable and a function of a discrete random variable
  \item
    conceptualize and compute the expected value of a linear function of
    \textbf{two or more} discrete random variable
  \item
    conceptualize and compute the expected value of a product of
    \textbf{two independent} discrete random variable
  \end{itemize}
\item
  After reading, take reading
  \href{https://forms.gle/Hguw2WYckTfeXiE89}{quiz 8}
\end{itemize}

\subsection*{Section 4.1 and 4.2}\label{section-4.1-and-4.2}
\addcontentsline{toc}{subsection}{Section 4.1 and 4.2}

Key terminology to know:

\begin{itemize}
\tightlist
\item[$\square$]
  Expectation, or expected value, of a random variable, denoted \(E(X)\)
  where \(X\) is the random variable
\item[$\square$]
  A function of a random variable, often denoted \(g(X)\), and its
  expectation
\item[$\square$]
  A \emph{linear} function of a random variable, denoted \(aX+b\), and
  its expectation
\end{itemize}

Comments:

\begin{itemize}
\tightlist
\item
  We will skip 4.3 for now and we'll formally cover joint pmf in a
  couple weeks.
\item
  Examples 4.3, 4.4, 4.8 use Series facts shown in Appendix C. Be
  comfortable using these facts to compute expectations.
\item
  The expectation of a linear function of a random variable is an
  \emph{extremely} useful fact so make sure you spend some time
  absorbing it. For example, suppose \(X\) counts the number of red
  lights in 5 days (ie it is Binomial). We might care more about
  \(X/5\), which is the \emph{proportion} of read lights in a week. If
  we know \(E(X)\), the average \emph{number} of red lights in a week,
  then we can easily compute \(E(X/5)\) using this rule.
\item
  The R code shows simulation examples that can be used to approximate
  an expected value. We won't be doing simulations this term but feel
  free to run the code if you want to learn how to do this for selected
  examples.
\end{itemize}

\subsection*{Section 4.4 and 4.5}\label{section-4.4-and-4.5}
\addcontentsline{toc}{subsection}{Section 4.4 and 4.5}

Key terminology to know:

\begin{itemize}
\tightlist
\item[$\square$]
  Expectation of the product of two independent random variables
\item[$\square$]
  Expectation of a linear combination of two or more random variables
\end{itemize}

Comments:

\begin{itemize}
\tightlist
\item
  For section 4.4, focus on the expectation rules for indep. RV in
  Equations 4.4 and 4.5. We will formally define and work with joint pmf
  later this term.
\item
  Skim Section 4.4.1. We will prove that sums of independent Poisson RVs
  is Poisson using a different method.
\item
  For section 4.5, focus on the linearity result at the start of the
  section but don't worry about the proof (for now)
\item
  The expectation of a linear function of \emph{two or more} random
  variable is an \emph{extremely} useful fact so make sure you spend
  some time absorbing it.
\end{itemize}

\section*{Day 11}\label{day-11}
\addcontentsline{toc}{section}{Day 11}

\markright{Day 11}

\begin{itemize}
\item
  What to read: Read sections 4.6, 5.2
\item
  Learning objectives: These sections will help you

  \begin{itemize}
  \tightlist
  \item
    conceptualize and compute the variance and standard deviation of a
    discrete random variable
  \item
    compute the variance and standard deviation of a linear function of
    a discrete random variable
  \item
    compute the variance and standard deviation of a linear function of
    two independent discrete random variables
  \item
    understand how moments of a random variable can be computed via the
    moment generating function
  \item
    understand how to use moment generating functions to determine the
    distribution of a function of one or more random variables
  \end{itemize}
\item
  After reading, take reading
  \href{https://forms.gle/oQ9ZMzGcAznhwoZa6}{quiz 9}
\end{itemize}

\subsection*{Section 4.6}\label{section-4.6}
\addcontentsline{toc}{subsection}{Section 4.6}

Key terminology to know:

\begin{itemize}
\tightlist
\item[$\square$]
  variance
\item[$\square$]
  standard deviation
\end{itemize}

Comments:

\begin{itemize}
\tightlist
\item
  Equation 4.10 will be the typical way we compute variance/standard
  deviation since it is less mathematically intensive to compute
  compared to the expected value used to define it.
\item
  Example 4.24 use Series facts shown in Appendix C.
\end{itemize}

\subsection*{Section 5.2}\label{section-5.2}
\addcontentsline{toc}{subsection}{Section 5.2}

Key terminology to know:

\begin{itemize}
\tightlist
\item[$\square$]
  the \(k\)th moments of a random variable
\item[$\square$]
  moment generating function (mgf)
\end{itemize}

Comments:

\begin{itemize}
\tightlist
\item
  We jump ahead a bit to cover moment generating functions (mgf) which
  can be used to find expectations of the form \(E(X^k)\) (the \(k\)th
  moment).
\item
  An \emph{extremely} useful fact about mgf's is that they uniquely
  identify a random variable, eg if two random variables have the same
  mgf then they have the same distribution. This combined with the
  properties described on pages 195-6 make mgf's a very useful tool for
  determining the distribution of linear combinations of one or more
  random variables.
\end{itemize}

\section*{Day 12}\label{day-12}
\addcontentsline{toc}{section}{Day 12}

\markright{Day 12}

\begin{itemize}
\item
  What to read: Read sections 5.1, 5.3, 5.4
\item
  Learning objectives: These sections will help you

  \begin{itemize}
  \tightlist
  \item
    get familiar with a few more common discrete RV types (geometric,
    negative binomial, and hypergeometric)
  \end{itemize}
\item
  After reading, take reading
  \href{https://forms.gle/BmZE2tNwiM2tcVmd8}{quiz 10}
\end{itemize}

\subsection*{Sections 5.1, 5.3, 5.4}\label{sections-5.1-5.3-5.4}
\addcontentsline{toc}{subsection}{Sections 5.1, 5.3, 5.4}

Key terminology to know:

\begin{itemize}
\tightlist
\item[$\square$]
  geometric RV (know shorthand notation, pmf and support set)
\item[$\square$]
  memorylessness of the geometric distribution
\item[$\square$]
  negative binomial RV (know shorthand notation, pmf and support set)
\item[$\square$]
  hypergeometric RV (know shorthand notation, pmf and support set)
\end{itemize}

Comments:

\begin{itemize}
\tightlist
\item
  skim 5.1.2
\item
  Take care to understand what the geometric and negative binomial
  random variables count. While the Binomial counts the number of
  ``successes'' in a fixed number of trials, the geometric/negative
  binomail fix the number of successes while the number of trials that
  it takes to get those successes is random.
\item
  Some probability books or online sources define geometric and negative
  binomial random variables as the number \emph{failures} until some
  fixed number of successes. Eg. if \(X\) is the number of trials until
  the 1st success, then \(Y = X-1\) is the number of failures until the
  1st success. We will stick with the number of \emph{trials} definition
  in this course.
\item
  A hypergeometric random variable is similar in spirit as the binomial,
  but the hypergeometric counts the number of successes in a fixed
  sample size taken \emph{without replacement} from a group of unique
  individuals. A binomial random variable is similar but it assumes with
  replacement, or independent sampling draw-to-draw.
\end{itemize}

\bookmarksetup{startatroot}

\chapter{Introduction to continuous random
variables}\label{sec-intro-cont-rv}

Chapter 6 and 7 of Wagaman (2021) introduces us to continuous random
variables.

Make sure to review our
\href{https://docs.google.com/spreadsheets/d/1p4s9FLap8CUB40UYAzdxpDi8-PkoHUceGq0dNUmfE9c/edit?usp=sharing}{week
5 schedule}.

\section*{Day 13}\label{day-13}
\addcontentsline{toc}{section}{Day 13}

\markright{Day 13}

\begin{itemize}
\item
  What to read: Read intro to ch.~6, sections 6.1, 6.2, 6.4, 7.1.3
\item
  Learning objectives: These sections will help you

  \begin{itemize}
  \tightlist
  \item
    understand what a continuous random variable is
  \item
    use a probability mass function (pdf) to compute probabilities for
    events involving a continuous RV
  \item
    use a cumulative distribution function (cdf) to compute
    probabilities for events involving a continuous RV
  \item
    understand how to get a pdf from a cdf and vice versa
  \item
    compute quantiles for a continuous distribution
  \item
    get familiar with a (continuous) uniform RV
  \end{itemize}
\item
  After reading, take reading
  \href{https://forms.gle/oZGaWtp9KkFa5MmCA}{quiz 11}
\end{itemize}

\subsection*{Section 6.1}\label{section-6.1}
\addcontentsline{toc}{subsection}{Section 6.1}

Key terminology to know:

\begin{itemize}
\tightlist
\item[$\square$]
  continuous random variable
\item[$\square$]
  probability density function (pdf) of a continuous RV
\end{itemize}

Comments:

\begin{itemize}
\tightlist
\item
  Pay particular attention to the comment box on page 230. I am very
  particular when asking for pdf (or cdf) functions, especially on
  exams. If you are asked to define a pdf/cdf \emph{over the entire real
  ine}, then you must get both the support \(S\) and ``0 otherwise''
  parts correct.
\item
  Yes, you will be integrating functions in this class! While I may ask
  you do to more involved calculations, eg integration by parts, on
  homework, I don't ask these longer calculation-focused questions on
  exams. Often (but not always) on exams, I'll ask you to appropriately
  ``set-up'' an integral but not actually complete the calculation.
\item
  You can check integration calculations using technology, but you can't
  let technology do all the work. See the AI/tech homework policy on the
  syllabus.
\end{itemize}

\subsection*{Section 6.2 and 7.1.3}\label{section-6.2-and-7.1.3}
\addcontentsline{toc}{subsection}{Section 6.2 and 7.1.3}

Key terminology to know:

\begin{itemize}
\tightlist
\item[$\square$]
  cumulative distribution function (cdf) of a continuous RV
\item[$\square$]
  quantile of a continuous RV
\end{itemize}

Comments:

\begin{itemize}
\tightlist
\item
  Take some time to make sure you see how pdf's and cdf's are related
  for continuous random variables.
\item
  Take a look at the cdf example for a discrete RV, but I find cdf most
  useful for continuous RV. It is essential that you can use/derive cdf
  for continuous RV, but not as important for discrete (in my class).
\item
  In 7.1.3, I just want you to review the definition of a quantile and
  how it relates to a cdf (6.2). You can skim the examples in 7.1.3, but
  we will formally introduce the normal distribution on another day.
\item
  I've seen quantile described either as a percentage or a
  proportion/probability. E.g. the 50th quantile or the 0.5 quantile.
  Your book defines it as the former, ``50th'', but be comfortable with
  either description.
\end{itemize}

\subsection*{Section 6.4}\label{section-6.4}
\addcontentsline{toc}{subsection}{Section 6.4}

Key terminology to know:

\begin{itemize}
\tightlist
\item[$\square$]
  uniform RV (know shorthand notation, basic properties)
\end{itemize}

Comments:

\begin{itemize}
\tightlist
\item
  You can just read up to Expectation and Variance on page 240. Review
  these after day 14's reading.
\item
  Be comfortable computing probabilities for uniform RV using both a
  calculus approach and geometric approach. There are pros/cons to each,
  depending on the problem.
\end{itemize}

\section*{Day 14}\label{day-14}
\addcontentsline{toc}{section}{Day 14}

\markright{Day 14}

\begin{itemize}
\item
  What to read: Read sections 6.3, 6.5, 7.1, 7.2, 7.4
\item
  Learning objectives: These sections will help you

  \begin{itemize}
  \tightlist
  \item
    compute the expected value, variance and sd of a continuous RV
  \item
    review important properties of expected values and variance/sd
  \item
    get familiar with some common continous RV types (exponential,
    normal, gamma, beta)
  \end{itemize}
\item
  After reading, take reading
  \href{https://forms.gle/UEE5pFNSJ3weXMBQ6}{quiz 12}
\end{itemize}

\subsection*{Section 6.3}\label{section-6.3}
\addcontentsline{toc}{subsection}{Section 6.3}

Key terminology to know:

\begin{itemize}
\tightlist
\item[$\square$]
  expected value of a continuous RV
\item[$\square$]
  variance/sd of a continuous RV
\end{itemize}

Comments:

\begin{itemize}
\tightlist
\item
  We'll cover 6.6-6.8 later this term
\end{itemize}

\subsection*{Section 6.5 and 7.2}\label{section-6.5-and-7.2}
\addcontentsline{toc}{subsection}{Section 6.5 and 7.2}

Key terminology to know:

\begin{itemize}
\tightlist
\item[$\square$]
  exponential RV (know shorthand notation, basic properties)
\item[$\square$]
  gamma RV (know shorthand notation, basic properties)
\item[$\square$]
  gamma function \(\Gamma\)
\end{itemize}

Comments:

\begin{itemize}
\tightlist
\item
  Exponential and gamma are common models for ``waiting times'', eg time
  until something happens.
\item
  Make sure you recognize the exponential as a special case of the gamma
\item
  We will cover the connection between these distributions and the
  Poisson distribution on another day
\end{itemize}

\subsection*{Section 7.1}\label{section-7.1}
\addcontentsline{toc}{subsection}{Section 7.1}

Key terminology to know:

\begin{itemize}
\tightlist
\item[$\square$]
  normal RV (know shorthand notation, basic properties)
\end{itemize}

Comments:

\begin{itemize}
\tightlist
\item
  Skip 7.1.2 (pages 278-282)
\end{itemize}

\subsection*{Section 7.4}\label{section-7.4}
\addcontentsline{toc}{subsection}{Section 7.4}

Key terminology to know:

\begin{itemize}
\tightlist
\item[$\square$]
  beta RV (know shorthand notation, basic properties)
\end{itemize}

Comments:

\begin{itemize}
\tightlist
\item
  The beta model is very useful for random variables that have a support
  between 0 and 1.
\end{itemize}

\section*{Day 15}\label{day-15}
\addcontentsline{toc}{section}{Day 15}

\markright{Day 15}

\begin{itemize}
\item
  What to read: Read sections 8.1
\item
  Learning objectives: This section will help you

  \begin{itemize}
  \tightlist
  \item
    use cdf's to derive the distribution (cdf, then pdf) of a function
    of a continuous RV
  \end{itemize}
\item
  No reading quiz on section 8.1
\end{itemize}

\subsection*{Section 8.1}\label{section-8.1}
\addcontentsline{toc}{subsection}{Section 8.1}

Key terminology to know:

\begin{itemize}
\tightlist
\item[$\square$]
  cdf and pdf (make sure these ideas are solid)
\end{itemize}

Comments:

\begin{itemize}
\tightlist
\item
  Skim 8.1.1 and skip 8.1.2
\item
  This section outlines how to derive the pdf of a function of a RV
  using the cdf method.
\item
  This method always ``works'', though often the mgf method is simpler
  \emph{if} the function is linear and the pdf distribution is a common
  one (eg normal, exponential, etc).
\end{itemize}

\bookmarksetup{startatroot}

\chapter{Poisson Process}\label{sec-intro-cont-rv}

Make sure to review our
\href{https://docs.google.com/spreadsheets/d/1qnz5t0OptIaeS-C-bjW4AMzgbrhckJPz4PuElSgGPac/edit?usp=sharing}{week
6 schedule}.

\section*{Day 16}\label{day-16}
\addcontentsline{toc}{section}{Day 16}

\markright{Day 16}

\begin{itemize}
\item
  What to read: Read section 7.3 through page 299.
\item
  Learning objectives: These sections will help you

  \begin{itemize}
  \tightlist
  \item
    understand what a Poisson process is
  \item
    see how ``interarrival'' or waiting times between Poisson events can
    be modeled by exponential or gamma distributed random variables
  \end{itemize}
\item
  After reading, take reading
  \href{https://forms.gle/8nwGZxx77bxcvn2f7}{quiz 13}
\end{itemize}

\subsection*{Section 7.3}\label{section-7.3}
\addcontentsline{toc}{subsection}{Section 7.3}

Key terminology to know:

\begin{itemize}
\tightlist
\item[$\square$]
  Poisson process with \(N_t \sim Pois(\lambda t)\)
\item[$\square$]
  Interarrival times \(E_k \sim Exp(\lambda)\)
\item[$\square$]
  Time until the \(nth\) event \(S_n \sim Gamma(n,\lambda)\)
\end{itemize}

Comments:

\begin{itemize}
\item
  You can skim/skip the proof on page 296 (up to example 7.15). This
  proof of the Poisson process model relies on joint pdf which we
  haven't covered yet.
\item
  There is a \textbf{typo} on page 298: The \emph{Stationary increments}
  property should state that ``For all \(0 < s,t\), \ldots{}'' instead
  of \(0 < s < t\)
\end{itemize}

\bookmarksetup{startatroot}

\chapter{Introduction to joint
distributions}\label{sec-intro-joint-dist}

We will revisit chapters 4 and 6 of Wagaman (2021) to describe joint
distributions that can explain/model bivariate relationships between two
(or more) random variables. We jump around from chapter 4 (discrete) and
chapter 6 (continuous) to cover this topic.

Make sure to review our
\href{https://docs.google.com/spreadsheets/d/10AP55TSo0bgqklDCOioCiiBYEZAyhIytcNe8qmd2nu8/edit?usp=sharing}{week
7 schedule}.

\section*{Day 18}\label{day-18}
\addcontentsline{toc}{section}{Day 18}

\markright{Day 18}

\begin{itemize}
\item
  What to read: Read sections 4.3, 6.6, review Appendix D if you need
  multivariate integration review
\item
  Learning objectives: These sections will help you

  \begin{itemize}
  \tightlist
  \item
    understand what a joint distribution (pdf/pmf) is
  \item
    understand what a marginal distribution (pdf/pmf) is
  \item
    compute a marginal pdf/pmf from a joint pdf/pmf
  \item
    compute probabilities from a joint pdf/pmf
  \end{itemize}
\item
  After reading, take reading
  \href{https://forms.gle/XKWqF5TZ1qVWDscE9}{quiz 14}
\end{itemize}

\subsection*{Sections 4.3 and 6.6}\label{sections-4.3-and-6.6}
\addcontentsline{toc}{subsection}{Sections 4.3 and 6.6}

Key terminology to know:

\begin{itemize}
\tightlist
\item[$\square$]
  joint pmf (two discrete RV) and joint pdf (two continuous RV)
\item[$\square$]
  marginal distribution (pmf or pdf)
\item[$\square$]
  continuous joint cdf
\item[$\square$]
  continuous bivariate pdf
\end{itemize}

Comments:

\begin{itemize}
\tightlist
\item
  You can skip over the expectations on page 139 (equation 4.3) and 255
  for now - we'll cover this idea Friday.
\item
  Computing probabilities from a joint pdf requires calculation of a
  double integral. As in single random variable integration, setting up
  the problem with the correct limits of integration is the key step
  here. \textbf{Draw a picture} of the event (region) that you want to
  find the probability of and \textbf{intersect} it with the support set
  for your pdf. It is this region that determines your limits!
\item
  Make sure you review Appendix D if you are rusty with double
  integrals.
\end{itemize}

\section*{Day 19}\label{day-19}
\addcontentsline{toc}{section}{Day 19}

\markright{Day 19}

\begin{itemize}
\item
  What to read: Read section 6.7, review independence from 4.4
\item
  Learning objectives: These sections will help you

  \begin{itemize}
  \tightlist
  \item
    understand what independence for two (or more) RV means
  \item
    how to \emph{prove} independence OR how to \emph{use} independence
    to construct joint pdf/pmf
  \end{itemize}
\item
  After reading, take reading
  \href{https://forms.gle/UvGofRr38VkZCCtf7}{quiz 15}
\end{itemize}

\subsection*{Sections 4.4 and 6.7}\label{sections-4.4-and-6.7}
\addcontentsline{toc}{subsection}{Sections 4.4 and 6.7}

Key terminology to know:

\begin{itemize}
\tightlist
\item[$\square$]
  independent continuous RV
\item[$\square$]
  independent discrete RV
\end{itemize}

Comments:

\begin{itemize}
\tightlist
\item
  You can skim 6.7.1 on the accept-reject method.
\item
  The assumption of independence between two RV makes the derivation of
  the joint pdf/pmf very easy since it is just the product of the two
  (marginal) pdf/pmf.
\end{itemize}

\section*{Day 20}\label{day-20}
\addcontentsline{toc}{section}{Day 20}

\markright{Day 20}

\begin{itemize}
\item
  What to read: Read expectation definitions on pages 139 and 255 and
  sections 4.7, 6.8, 9.6
\item
  Learning objectives: These sections will help you

  \begin{itemize}
  \tightlist
  \item
    compute the expected value of a function of two discrete or
    continuous RV
  \item
    understand how covariance and correlation are used to measure the
    \emph{linear} dependence between two RV
  \item
    compute the covariance and correlation between two RV
  \item
    compute the variance of a linear combination of two RV when they are
    not independent
  \item
    see an example of a bivariate distribution with a parameter that
    measures correlation
  \end{itemize}
\item
  After reading, take reading
  \href{https://forms.gle/b5vx8Ezd2b5SB2Jz9}{quiz 16}
\end{itemize}

\subsection*{Sections 4.7, 6.8, and 9.6 and pages 139,
255}\label{sections-4.7-6.8-and-9.6-and-pages-139-255}
\addcontentsline{toc}{subsection}{Sections 4.7, 6.8, and 9.6 and pages
139, 255}

Key terminology to know:

\begin{itemize}
\tightlist
\item[$\square$]
  expected value of a function of two RV
\item[$\square$]
  covariance
\item[$\square$]
  correlation
\item[$\square$]
  general formula for the variance of a sum of random variables
\item[$\square$]
  bivariate normal model
\end{itemize}

Comments:

\begin{itemize}
\tightlist
\item
  Take a look at section 9.6 through page 391 to see an example of a
  joint pdf with a parameter that measures correlation.
\end{itemize}

\bookmarksetup{startatroot}

\chapter{More with joint and conditional
distributions}\label{sec-more-joint-dist}

We will explore strategies for deriving the distribution of a function
of a two (or more) random variables. Then we return to thinking
conditionally (first introduced in Chapter~\ref{sec-think-cond}) to see
how to define a conditional pdf/pmf probability model.

Make sure to review our
\href{https://docs.google.com/spreadsheets/d/1RrtZ7JiEBz_r4XMC7Eib2ObRZpGSXkg2fVV1h8YrnUw/edit?usp=sharing}{week
8 schedule}.

\section*{Day 21}\label{day-21}
\addcontentsline{toc}{section}{Day 21}

\markright{Day 21}

\begin{itemize}
\item
  What to read: Read sections 8.2, 8.3, 8.4
\item
  Learning objectives: These sections will help you

  \begin{itemize}
  \tightlist
  \item
    derive the pdf of a minimum or maximum
  \item
    derive the pmf/pdf of a sum of two independent RV using a
    convolution of functions
  \end{itemize}
\item
  After reading, take reading
  \href{https://forms.gle/KsqVyoKtsQLFrZcs5}{quiz 17}
\end{itemize}

\subsection*{Sections 8.2}\label{sections-8.2}
\addcontentsline{toc}{subsection}{Sections 8.2}

Key terminology to know:

\begin{itemize}
\tightlist
\item[$\square$]
  order statistics
\item[$\square$]
  inequalities for mins and maxs
\end{itemize}

Comments:

\begin{itemize}
\tightlist
\item
  Focus on material involving the min \(X_{(1)}\) and max \(X_{(n)}\)
  order statistics.
\item
  You can skim the proof of the beta distribution for general
  \(X_{(k)}\) for \(1 < k < n\) when your sample is iid uniform. (pages
  333-334)
\end{itemize}

\subsection*{Sections 8.3}\label{sections-8.3}
\addcontentsline{toc}{subsection}{Sections 8.3}

Key terminology to know:

\begin{itemize}
\tightlist
\item[$\square$]
  convolution of two functions
\end{itemize}

Comments:

\begin{itemize}
\tightlist
\item
  The convolution method of finding the pdf of \(X+Y\) is an example of
  a CDF transformation method (i.e.~find the CDF then differentiate to
  find PDF)
\item
  Review section 4.4.1 equation 4.7 for review of the analogous discrete
  version of 8.3.
\end{itemize}

\subsection*{Sections 8.4}\label{sections-8.4}
\addcontentsline{toc}{subsection}{Sections 8.4}

Comments:

\begin{itemize}
\tightlist
\item
  This section reviews both how to use geometry to compute probabilities
  from uniform pdfs and use of the CDF method to find PDF for a function
  of two RV.
\item
  You can skim 8.23 (Buffon's needle problem is fun but you won't
  necessarily see trig calculations in hw/exam problems!)
\end{itemize}

\section*{Day 22}\label{day-22}
\addcontentsline{toc}{section}{Day 22}

\markright{Day 22}

\begin{itemize}
\item
  What to read: Read sections 4.8, 9.1, 9.2
\item
  Learning objectives: These sections will help you

  \begin{itemize}
  \tightlist
  \item
    derive conditional pmf/pdf from joint and marginal models
  \item
    apply Bayes formula to conditional pmf/pdf
  \end{itemize}
\item
  After reading, take reading
  \href{https://forms.gle/aoCcCr7x9s8o6Uhk8}{quiz 18}
\end{itemize}

\subsection*{Sections 4.3, 9.1, and 9.2}\label{sections-4.3-9.1-and-9.2}
\addcontentsline{toc}{subsection}{Sections 4.3, 9.1, and 9.2}

Key terminology to know:

\begin{itemize}
\tightlist
\item[$\square$]
  conditional pmf
\item[$\square$]
  conditional pdf
\item[$\square$]
  Bayes formula for pdf
\end{itemize}

Comments:

\begin{itemize}
\tightlist
\item
  We will cover expected value in 4.8.1 on Day 23.
\item
  Take care when deriving conditional pmf/pdf. E.g the pdf
  \(f(y \mid x)\) of \(Y \mid X=x\) is a function of \(y\) and \(x\) is
  a constant.
\end{itemize}

\section*{Day 23}\label{day-23}
\addcontentsline{toc}{section}{Day 23}

\markright{Day 23}

\begin{itemize}
\item
  What to read: Read sections 9.3, 9.5
\item
  Learning objectives: These sections will help you

  \begin{itemize}
  \tightlist
  \item
    Compute a conditional pmf/pdf's expected value and variance given a
    fixed value of the conditioning variable
  \item
    Compute a marginal expected value or variance from a RV's
    conditional distribution
  \end{itemize}
\item
  After reading, take reading
  \href{https://forms.gle/AZmecHbJtKckUvkL6}{quiz 19}
\end{itemize}

\subsection*{Sections 9.3 and 9.5}\label{sections-9.3-and-9.5}
\addcontentsline{toc}{subsection}{Sections 9.3 and 9.5}

Key terminology to know:

\begin{itemize}
\tightlist
\item[$\square$]
  Conditional expected value of \(Y\) given \(X=x\): \(E(Y \mid X=x)\)
  (similar for flipping the roles of \(X\) and \(Y\))
\item[$\square$]
  Conditional variance of \(Y\) given \(X=x\): \(V(Y \mid X=x)\)
  (similar for flipping the roles of \(X\) and \(Y\))
\item[$\square$]
  LOTUS for conditional distributions
\item[$\square$]
  Conditional expected value of \(Y\) given \(X\): \(E(Y \mid X)\)
  (similar for flipping the roles of \(X\) and \(Y\))
\item[$\square$]
  Conditional variance of \(Y\) given \(X\): \(V(Y \mid X)\) (similar
  for flipping the roles of \(X\) and \(Y\))
\item[$\square$]
  Law of total expectation
\item[$\square$]
  Law of total variance
\end{itemize}

Comments:

\begin{itemize}
\item
  For these sections, we are framing the conditional relation as ``\(Y\)
  given \(X\)''. The same rules/def/ideas apply if we flip the roles of
  \(Y\) and \(X\), so don't get stuck on a particular formula - try to
  see how things would generalize to any RV. E.g. examples 9.13 and 9.14
  switch up notation.
\item
  For these sections, we often frame the known information/models as
  being given the distribution (marginally) of \(X\) and the conditional
  distribution of \(Y\) given \(X=x\) (or \(X\)). As already mentioned,
  we could flip the roles of the RVs and have the same general rules
  apply.
\item
  I know these sections can get confusing since we are talking about
  very similar things with similar notation. One important idea, though,
  is whether a value (exp/var) is \emph{fixed} (but maybe unknown, in
  formula form) or \emph{random.}

  \begin{itemize}
  \tightlist
  \item
    \(E(Y \mid X=x)\) assumes that we know a value of \(X=x\). The final
    answer may be a function of \(x\), but \(x\) is a fixed (not random)
    value. Like in Ex 9.12, \(E(Y \mid X=0.6) = (0.6)/2=0.3\) is the
    expected value of Miguel's number if Riley picks \(X=0.6\).
  \item
    \(E(Y \mid X) = g(X)\) keeps \(X\) \emph{random}. Why do this? We
    might want to know how \(E(Y \mid X) = g(X)\) behaves over all
    possible, random, values of \(X\). Like in Ex 9.12,
    \(E(Y \mid X) = X/2\) tells us that the random variable \(X/2\)
    models the expected value of Miguel's number as a function of the
    random value \(X\) of Riley's pick.
  \end{itemize}
\item
  The law of total expectation tells us the ``average'' value of
  \(E(Y \mid X)\) over all values of \(X\) (``averaged'' over the
  pdf/pmf of \(X\)). It is a way to get the unconditional expectation of
  \(Y\) without having to compute the marginal distribution of \(Y\).

  \begin{itemize}
  \tightlist
  \item
    e.g.~in Ex 9.12, \(E(Y \mid X) = X/2\) tells us the expected value
    of Miguel's number as a function of the random value \(X\). If
    \(X \sim Unif(0,1)\), then \(E(X) = 0.5\) and the unconditional
    expectation for Miguel's number is
    \(E(Y) = E_x(E(Y \mid X)) = E_x(X/2) = (0.5)/2 = 0.25\). As a gut
    check, we should expect the expectation for Miguel to be lower than
    Riley because Riley's number always dictates the upper bound on
    Miguel's number.
  \end{itemize}
\item
  The law of total variance (LOTV) is similar in spirit to the law of
  total expectation, that we want to understand the variance of \(Y\)
  unconditionally when what we are given is the conditional variance.
  But, the equation for the LOTV is more complex because the variance
  term is more complex: it involves taking an expectation of squared
  deviation of the RV from its expectation. The LOTV is computed with
  two parts:

  \begin{itemize}
  \tightlist
  \item
    \(E_x(V(Y \mid X))\): ``average'' the conditional variance (RV)
    \(V(Y\mid X)\) over all values of \(X\) (``averaged'' over the
    pdf/pmf of \(X\)).
  \item
    \(V_x(E(Y \mid X))\): measure the variation of the conditional
    expected value (RV) \(E(Y\mid X)\) over all values of \(X\)
    (variation with respect to the pdf/pmf of \(X\)).
  \end{itemize}
\end{itemize}

\bookmarksetup{startatroot}

\chapter{Some Limit Theorems}\label{sec-limit-thms}

We will explore strategies for deriving the distribution of a function
of a two (or more) random variables. Then we return to thinking
conditionally (first introduced in Chapter~\ref{sec-think-cond}) to see
how to define a conditional pdf/pmf probability model.

Make sure to review our
\href{https://docs.google.com/spreadsheets/d/1RrtZ7JiEBz_r4XMC7Eib2ObRZpGSXkg2fVV1h8YrnUw/edit?usp=sharing}{week
8 schedule}.

\section*{Day 26}\label{day-26}
\addcontentsline{toc}{section}{Day 26}

\markright{Day 26}

\begin{itemize}
\item
  What to read: Read sections 10.5 and 10.6 but skip section 10.5.1
\item
  Learning objectives: These sections will help you

  \begin{itemize}
  \tightlist
  \item
    understand and apply the Central Limit Theorem (CLT)
  \item
    understand how the CLT is proven with MGF
  \end{itemize}
\item
  After reading, take reading
  \href{https://forms.gle/6GziRQC46YbXrSE79}{quiz 20}
\end{itemize}

\subsection*{Sections 10.5 and 10.6}\label{sections-10.5-and-10.6}
\addcontentsline{toc}{subsection}{Sections 10.5 and 10.6}

Key terminology to know:

\begin{itemize}
\tightlist
\item[$\square$]
  sum \(S_n\) of \(n\) i.i.d. random variables
\item[$\square$]
  mean \(\bar{X}_n\) of \(n\) i.i.d. random variables
\end{itemize}

Comments:

\begin{itemize}
\tightlist
\item
  We will cover the proof of the CLT in 10.6 in class
\end{itemize}

\section*{Day 27}\label{day-27}
\addcontentsline{toc}{section}{Day 27}

\markright{Day 27}

\begin{itemize}
\item
  What to read: Read sections 10.2 and 10.4
\item
  Learning objectives: These sections will help you

  \begin{itemize}
  \tightlist
  \item
    understand the big picture implications of the Law of Large Numbers
    (LLN)
  \item
    understand how Monte Carlo integration is an application of the LLN
  \end{itemize}
\end{itemize}

\subsection*{Sections 10.2 and 10.4}\label{sections-10.2-and-10.4}
\addcontentsline{toc}{subsection}{Sections 10.2 and 10.4}

Key terminology to know:

\begin{itemize}
\tightlist
\item[$\square$]
  probability of a limit
\item[$\square$]
  Monte Carlo integration
\end{itemize}

Comments:

\begin{itemize}
\tightlist
\item
  Focus on the LLN big picture in 10.2
\item
  We will use the LLN to justify an important application: Monte Carlo
  integration
\end{itemize}

\bookmarksetup{startatroot}

\chapter*{References}\label{references}
\addcontentsline{toc}{chapter}{References}

\markboth{References}{References}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-dobrow}
Wagaman, \& Dobrow, A. S. 2021. \emph{Probability: With Applications and
r}. 2nd ed. Wiley.

\end{CSLReferences}


\backmatter


\end{document}
